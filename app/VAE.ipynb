{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05e5102-fad5-40ad-9727-8eea836a4624",
   "metadata": {},
   "source": [
    "# 必要なパッケージのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae682b97-0e6b-4512-9075-59e8d8256700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # tensorboardの出力先作成\n",
    "import matplotlib.pyplot as plt # 可視化\n",
    "import numpy as np # 計算\n",
    "import torch # 機械学習フレームワークとしてpytorchを使用\n",
    "import torch.nn as nn # クラス内で利用するモジュールのため簡略化\n",
    "import torch.nn.functional as F # クラス内で利用するモジュールのため簡略化\n",
    "from torch import optim # 最適化アルゴリズム\n",
    "from torch.utils.tensorboard import SummaryWriter # tensorboardの利用\n",
    "from torchvision import datasets, transforms # データセットの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e23020-f235-4023-87c2-4bbdd65376b8",
   "metadata": {},
   "source": [
    "# GPU使用の手続き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106f83ba-d3df-45a7-92c4-0b941eebaa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUが使える場合はGPU上で動かす\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# tensorboardのログの保存先\n",
    "if not os.path.exists(\"./logs\"):\n",
    "    os.makedirs(\"./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ebcd0e-fbc1-4139-a443-af8ac128cf2d",
   "metadata": {},
   "source": [
    "# データセットの取得、加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7aca32-5c11-44e1-a096-1bc6ecb87f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNISTのデータをとってくるときに一次元化する前処理\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "# trainデータとtestデータに分けてデータセットを取得\n",
    "dataset_train_valid = datasets.MNIST(\"./\", train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.MNIST(\"./\", train=False, download=True, transform=transform)\n",
    "# trainデータの20%はvalidationデータとして利用\n",
    "size_train_valid = len(dataset_train_valid) # 60000\n",
    "size_train = int(size_train_valid * 0.8) # 48000\n",
    "size_valid = size_train_valid - size_train # 12000\n",
    "dataset_train, dataset_valid = torch.utils.data.random_split(dataset_train_valid, [size_train, size_valid])\n",
    "# 取得したデータセットをDataLoader化する\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=1000, shuffle=True)\n",
    "dataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=1000, shuffle=False)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6796d-d1fc-4eee-b9b6-464ded68dfe3",
   "metadata": {},
   "source": [
    "# VAEモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4bfb180-a2d8-400c-a5b1-81ea157855af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        \"\"\"コンストラクタ\n",
    "\n",
    "        Args:\n",
    "            z_dim (int): 潜在空間の次元数\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "\n",
    "        Note:\n",
    "            eps (float): オーバーフローとアンダーフローを防ぐための微小量\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__() # VAEクラスはnn.Moduleを継承しているため親クラスのコンストラクタを呼ぶ必要がある\n",
    "        self.eps = np.spacing(1) # オーバーフローとアンダーフローを防ぐための微小量\n",
    "        self.x_dim = 28 * 28 # MNISTの場合は28×28の画像であるため\n",
    "        self.z_dim = z_dim # インスタンス化の際に潜在空間の次元数は自由に設定できる\n",
    "        self.enc_fc1 = nn.Linear(self.x_dim, 400) # エンコーダ1層目\n",
    "        self.enc_fc2 = nn.Linear(400, 200) # エンコーダ2層目\n",
    "        self.enc_fc3_mean = nn.Linear(200, z_dim) # 近似事後分布の平均\n",
    "        self.enc_fc3_logvar = nn.Linear(200, z_dim) # 近似事後分布の分散の対数\n",
    "        self.dec_fc1 = nn.Linear(z_dim, 200) # デコーダ1層目\n",
    "        self.dec_fc2 = nn.Linear(200, 400) # デコーダ2層目\n",
    "        self.dec_drop = nn.Dropout(p=0.2) # 過学習を防ぐために最終層の直前にドロップアウト\n",
    "        self.dec_fc3 = nn.Linear(400, self.x_dim) # デコーダ3層目\n",
    "\n",
    "    def encoder(self, x):\n",
    "        \"\"\"エンコーダ\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): (バッチサイズ, 入力次元数)サイズの入力データ\n",
    "\n",
    "        Returns:\n",
    "            mean (torch.tensor): 近似事後分布の平均\n",
    "            logvar (torch.tensor): 近似事後分布の分散の対数\n",
    "        \"\"\"\n",
    "        x = F.relu(self.enc_fc1(x))\n",
    "        x = F.relu(self.enc_fc2(x))\n",
    "        return self.enc_fc3_mean(x), self.enc_fc3_logvar(x)\n",
    "\n",
    "    def sample_z(self, mean, log_var, device):\n",
    "        \"\"\"Reparameterization trickに基づく潜在変数Zの疑似的なサンプリング\n",
    "\n",
    "        Args:\n",
    "            mean (torch.tensor): 近似事後分布の平均\n",
    "            logvar (torch.tensor): 近似事後分布の分散の対数\n",
    "            device (String): GPUが使える場合は\"cuda\"でそれ以外は\"cpu\"\n",
    "\n",
    "        Returns:\n",
    "            z (torch.tensor): (バッチサイズ, z_dim)サイズの潜在変数\n",
    "        \"\"\"\n",
    "        epsilon = torch.randn(mean.shape, device=device)\n",
    "        return mean + epsilon * torch.exp(0.5 * log_var)\n",
    "\n",
    "    def decoder(self, z):\n",
    "        \"\"\"デコーダ\n",
    "\n",
    "        Args:\n",
    "            z (torch.tensor): (バッチサイズ, z_dim)サイズの潜在変数\n",
    "\n",
    "        Returns:\n",
    "            y (torch.tensor): (バッチサイズ, 入力次元数)サイズの再構成データ\n",
    "        \"\"\"\n",
    "        z = F.relu(self.dec_fc1(z))\n",
    "        z = F.relu(self.dec_fc2(z))\n",
    "        z = self.dec_drop(z)\n",
    "        return torch.sigmoid(self.dec_fc3(z))\n",
    "\n",
    "    def forward(self, x, device):\n",
    "        \"\"\"順伝播処理\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): (バッチサイズ, 入力次元数)サイズの入力データ\n",
    "            device (String): GPUが使える場合は\"cuda\"でそれ以外は\"cpu\"\n",
    "\n",
    "        Returns:\n",
    "            KL (torch.float): KLダイバージェンス\n",
    "            reconstruction (torch.float): 再構成誤差\n",
    "            z (torch.tensor): (バッチサイズ, z_dim)サイズの潜在変数\n",
    "            y (torch.tensor): (バッチサイズ, 入力次元数)サイズの再構成データ            \n",
    "        \"\"\"\n",
    "        mean, log_var = self.encoder(x.to(device)) # encoder部分\n",
    "        z = self.sample_z(mean, log_var, device) # Reparameterization trick部分\n",
    "        y = self.decoder(z.to(device)).to(device) # decoder部分\n",
    "        KL = 0.5 * torch.sum(1 + log_var - mean**2 - torch.exp(log_var)) # KLダイバージェンス計算\n",
    "        reconstruction = torch.sum(x.to(device) * torch.log(y.to(device) + self.eps) + (1 - x.to(device)) * torch.log(1 - y.to(device) + self.eps)) # 再構成誤差計算\n",
    "        return [KL, reconstruction], z, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e7ec0-8fdc-4c33-a6c1-af9b6880df96",
   "metadata": {},
   "source": [
    "# ハイパーパラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b54427-b71c-4ecd-868e-a36c4e289444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAEクラスのコンストラクタに潜在変数の次元数を渡す\n",
    "model = VAE(2).to(device)\n",
    "# 今回はoptimizerとしてAdamを利用\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# 最大更新回数は1000回\n",
    "num_epochs = 1000\n",
    "# 検証データのロスとその最小値を保持するための変数を十分大きな値で初期化しておく\n",
    "loss_valid = 10 ** 7\n",
    "loss_valid_min = 10 ** 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf62862-fed7-4c05-8d15-b2713ea288b0",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50c3d7-18d7-438d-8875-4cce6913fe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 10:07:32.810119: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 10:07:33.286502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH1] loss_valid: 203274 | Loss_valid_min: 203274\n",
      "[EPOCH2] loss_valid: 188737 | Loss_valid_min: 188737\n",
      "[EPOCH3] loss_valid: 178321 | Loss_valid_min: 178321\n",
      "[EPOCH4] loss_valid: 173632 | Loss_valid_min: 173632\n",
      "[EPOCH5] loss_valid: 170099 | Loss_valid_min: 170099\n",
      "[EPOCH6] loss_valid: 166915 | Loss_valid_min: 166915\n",
      "[EPOCH7] loss_valid: 164262 | Loss_valid_min: 164262\n",
      "[EPOCH8] loss_valid: 162535 | Loss_valid_min: 162535\n",
      "[EPOCH9] loss_valid: 161106 | Loss_valid_min: 161106\n",
      "[EPOCH10] loss_valid: 159697 | Loss_valid_min: 159697\n",
      "[EPOCH11] loss_valid: 158674 | Loss_valid_min: 158674\n",
      "[EPOCH12] loss_valid: 157517 | Loss_valid_min: 157517\n",
      "[EPOCH13] loss_valid: 156669 | Loss_valid_min: 156669\n",
      "[EPOCH14] loss_valid: 155969 | Loss_valid_min: 155969\n",
      "[EPOCH15] loss_valid: 155125 | Loss_valid_min: 155125\n",
      "[EPOCH16] loss_valid: 154336 | Loss_valid_min: 154336\n",
      "[EPOCH17] loss_valid: 153597 | Loss_valid_min: 153597\n",
      "[EPOCH18] loss_valid: 153139 | Loss_valid_min: 153139\n",
      "[EPOCH19] loss_valid: 152702 | Loss_valid_min: 152702\n",
      "[EPOCH20] loss_valid: 152026 | Loss_valid_min: 152026\n",
      "[EPOCH21] loss_valid: 151424 | Loss_valid_min: 151424\n",
      "[EPOCH22] loss_valid: 151012 | Loss_valid_min: 151012\n",
      "[EPOCH23] loss_valid: 150581 | Loss_valid_min: 150581\n",
      "[EPOCH24] loss_valid: 150247 | Loss_valid_min: 150247\n",
      "[EPOCH25] loss_valid: 149954 | Loss_valid_min: 149954\n",
      "[EPOCH26] loss_valid: 149438 | Loss_valid_min: 149438\n",
      "[EPOCH27] loss_valid: 149445 | Loss_valid_min: 149438\n",
      "1回連続でValidationが悪化しました\n",
      "[EPOCH28] loss_valid: 148971 | Loss_valid_min: 148971\n",
      "[EPOCH29] loss_valid: 148793 | Loss_valid_min: 148793\n",
      "[EPOCH30] loss_valid: 148700 | Loss_valid_min: 148700\n",
      "[EPOCH31] loss_valid: 148362 | Loss_valid_min: 148362\n",
      "[EPOCH32] loss_valid: 148189 | Loss_valid_min: 148189\n",
      "[EPOCH33] loss_valid: 147845 | Loss_valid_min: 147845\n",
      "[EPOCH34] loss_valid: 147747 | Loss_valid_min: 147747\n",
      "[EPOCH35] loss_valid: 147693 | Loss_valid_min: 147693\n",
      "[EPOCH36] loss_valid: 147499 | Loss_valid_min: 147499\n",
      "[EPOCH37] loss_valid: 147587 | Loss_valid_min: 147499\n",
      "1回連続でValidationが悪化しました\n",
      "[EPOCH38] loss_valid: 147361 | Loss_valid_min: 147361\n",
      "[EPOCH39] loss_valid: 147272 | Loss_valid_min: 147272\n",
      "[EPOCH40] loss_valid: 146959 | Loss_valid_min: 146959\n",
      "[EPOCH41] loss_valid: 146755 | Loss_valid_min: 146755\n",
      "[EPOCH42] loss_valid: 146828 | Loss_valid_min: 146755\n",
      "1回連続でValidationが悪化しました\n",
      "[EPOCH43] loss_valid: 146667 | Loss_valid_min: 146667\n",
      "[EPOCH44] loss_valid: 146652 | Loss_valid_min: 146652\n",
      "[EPOCH45] loss_valid: 146513 | Loss_valid_min: 146513\n",
      "[EPOCH46] loss_valid: 146296 | Loss_valid_min: 146296\n",
      "[EPOCH47] loss_valid: 146237 | Loss_valid_min: 146237\n",
      "[EPOCH48] loss_valid: 146032 | Loss_valid_min: 146032\n",
      "[EPOCH49] loss_valid: 145984 | Loss_valid_min: 145984\n",
      "[EPOCH50] loss_valid: 145928 | Loss_valid_min: 145928\n",
      "[EPOCH51] loss_valid: 145808 | Loss_valid_min: 145808\n",
      "[EPOCH52] loss_valid: 145726 | Loss_valid_min: 145726\n",
      "[EPOCH53] loss_valid: 145573 | Loss_valid_min: 145573\n",
      "[EPOCH54] loss_valid: 145557 | Loss_valid_min: 145557\n",
      "[EPOCH55] loss_valid: 145737 | Loss_valid_min: 145557\n",
      "1回連続でValidationが悪化しました\n",
      "[EPOCH56] loss_valid: 145373 | Loss_valid_min: 145373\n",
      "[EPOCH57] loss_valid: 145408 | Loss_valid_min: 145373\n",
      "1回連続でValidationが悪化しました\n",
      "[EPOCH58] loss_valid: 145203 | Loss_valid_min: 145203\n",
      "[EPOCH59] loss_valid: 145133 | Loss_valid_min: 145133\n",
      "[EPOCH60] loss_valid: 145072 | Loss_valid_min: 145072\n",
      "[EPOCH61] loss_valid: 144925 | Loss_valid_min: 144925\n",
      "[EPOCH62] loss_valid: 144959 | Loss_valid_min: 144925\n",
      "1回連続でValidationが悪化しました\n",
      "[EPOCH63] loss_valid: 144823 | Loss_valid_min: 144823\n",
      "[EPOCH64] loss_valid: 144881 | Loss_valid_min: 144823\n",
      "1回連続でValidationが悪化しました\n",
      "[EPOCH65] loss_valid: 144757 | Loss_valid_min: 144757\n",
      "[EPOCH66] loss_valid: 144729 | Loss_valid_min: 144729\n",
      "[EPOCH67] loss_valid: 144505 | Loss_valid_min: 144505\n",
      "[EPOCH68] loss_valid: 144392 | Loss_valid_min: 144392\n",
      "[EPOCH69] loss_valid: 144519 | Loss_valid_min: 144392\n",
      "1回連続でValidationが悪化しました\n",
      "[EPOCH70] loss_valid: 144743 | Loss_valid_min: 144392\n",
      "2回連続でValidationが悪化しました\n",
      "[EPOCH71] loss_valid: 144376 | Loss_valid_min: 144376\n",
      "[EPOCH72] loss_valid: 144349 | Loss_valid_min: 144349\n",
      "[EPOCH73] loss_valid: 144358 | Loss_valid_min: 144349\n",
      "1回連続でValidationが悪化しました\n",
      "[EPOCH74] loss_valid: 144246 | Loss_valid_min: 144246\n",
      "[EPOCH75] loss_valid: 144214 | Loss_valid_min: 144214\n",
      "[EPOCH76] loss_valid: 144094 | Loss_valid_min: 144094\n",
      "[EPOCH77] loss_valid: 144208 | Loss_valid_min: 144094\n",
      "1回連続でValidationが悪化しました\n",
      "[EPOCH78] loss_valid: 144159 | Loss_valid_min: 144094\n",
      "2回連続でValidationが悪化しました\n",
      "[EPOCH79] loss_valid: 144046 | Loss_valid_min: 144046\n",
      "[EPOCH80] loss_valid: 144129 | Loss_valid_min: 144046\n",
      "1回連続でValidationが悪化しました\n"
     ]
    }
   ],
   "source": [
    "# early stoppingを判断するためのカウンタ変数\n",
    "num_no_improved = 0\n",
    "# tensorboardに記録するためのカウンタ変数\n",
    "num_batch_train = 0\n",
    "num_batch_valid = 0\n",
    "# tensorboardでモニタリングする\n",
    "writer = SummaryWriter(log_dir=\"./logs\")\n",
    "# 学習開始\n",
    "for num_iter in range(num_epochs):\n",
    "    model.train() # 学習前は忘れずにtrainモードにしておく\n",
    "    for x, t in dataloader_train: # dataloaderから訓練データを抽出する\n",
    "        lower_bound, _, _ = model(x, device) # VAEにデータを流し込む\n",
    "        loss = -sum(lower_bound) # lossは負の下限\n",
    "        model.zero_grad() # 訓練時のpytorchのお作法\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"Loss_train/KL\", -lower_bound[0].cpu().detach().numpy(), num_iter + num_batch_train)\n",
    "        writer.add_scalar(\"Loss_train/Reconst\", -lower_bound[1].cpu().detach().numpy(), num_iter + num_batch_train)\n",
    "        num_batch_train += 1\n",
    "    num_batch_train -= 1 # 次回のエポックでつじつまを合わせるための調整\n",
    "\n",
    "    # 検証開始\n",
    "    model.eval() # 検証前は忘れずにevalモードにしておく\n",
    "    loss = []\n",
    "    for x, t in dataloader_valid: # dataloaderから検証データを抽出する\n",
    "        lower_bound, _, _ = model(x, device) # VAEにデータを流し込む\n",
    "        loss.append(-sum(lower_bound).cpu().detach().numpy())\n",
    "        writer.add_scalar(\"Loss_valid/KL\", -lower_bound[0].cpu().detach().numpy(), num_iter + num_batch_valid)\n",
    "        writer.add_scalar(\"Loss_valid/Reconst\", -lower_bound[1].cpu().detach().numpy(), num_iter + num_batch_valid)\n",
    "        num_batch_valid += 1\n",
    "    num_batch_valid -= 1 # 次回のエポックでつじつまを合わせるための調整\n",
    "    loss_valid = np.mean(loss)\n",
    "    loss_valid_min = np.minimum(loss_valid_min, loss_valid)\n",
    "    print(f\"[EPOCH{num_iter + 1}] loss_valid: {int(loss_valid)} | Loss_valid_min: {int(loss_valid_min)}\")\n",
    "\n",
    "    # もし今までのlossの最小値よりも今回のイテレーxションのlossが大きければカウンタ変数をインクリメントする\n",
    "    if loss_valid_min < loss_valid:\n",
    "        num_no_improved += 1\n",
    "        print(f\"{num_no_improved}回連続でValidationが悪化しました\")\n",
    "    # もし今までのlossの最小値よりも今回のイテレーションのlossが同じか小さければカウンタ変数をリセットする\n",
    "    else:\n",
    "        num_no_improved = 0\n",
    "        torch.save(model.state_dict(), f\"./z_{model.z_dim}.pth\")\n",
    "    # カウンタ変数が10回に到達したらearly stopping\n",
    "    if (num_no_improved >= 10):\n",
    "        print(f\"{num_no_improved}回連続でValidationが悪化したため学習を止めます\")\n",
    "        break\n",
    "# tensorboardのモニタリングも停止しておく\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef30ea-c28e-4748-9237-65bddc723224",
   "metadata": {},
   "source": [
    "# tensorboardでの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2c09c-3f11-4bfa-8420-fdcdd87329fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir ./logs --port 6003 --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4342858-8dc3-43cd-8755-ae302cda4e13",
   "metadata": {},
   "source": [
    "# 潜在変数の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d929398-6dc6-4b23-b914-b94a51dc8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 2\n",
    "model = VAE(z_dim)\n",
    "model.load_state_dict(torch.load(\"./z_2.pth\"))\n",
    "cm = plt.get_cmap(\"tab10\") # カラーマップの用意\n",
    "# 可視化開始\n",
    "for num_batch, data in enumerate(dataloader_test):\n",
    "    fig_plot, ax_plot = plt.subplots(figsize=(9, 9))\n",
    "    fig_scatter, ax_scatter = plt.subplots(figsize=(9, 9))\n",
    "    # 学習済みVAEに入力を与えたときの潜在変数を抽出\n",
    "    _, z, _ = model(data[0], device)\n",
    "    z = z.detach().numpy()\n",
    "    # 各クラスごとに可視化する\n",
    "    for k in range(10):\n",
    "        cluster_indexes = np.where(data[1].detach().numpy() == k)[0]\n",
    "        ax_plot.plot(z[cluster_indexes,0], z[cluster_indexes,1], \"o\", ms=4, color=cm(k))\n",
    "    fig_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f1d04-a384-4cda-bace-cb5f7d839cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
